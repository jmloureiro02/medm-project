{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import researchpy as rp\n",
    "from scipy import stats\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score,precision_score, recall_score, roc_auc_score, log_loss, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier, GradientBoostingClassifier,BaggingClassifier,StackingClassifier\n",
    "\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file.\n",
    "csv_file = 'in-vehicle-coupon-recommendation.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame.\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.drop_duplicates()\n",
    "print(\"Shape of dataset after removing duplicates:\",df.shape)\n",
    "\n",
    "# Drop the 'car' , 'toCoupon_GEQ5min' and 'direction_opp' columns\n",
    "df = df.drop(['car', 'toCoupon_GEQ5min','direction_opp'], axis=1)\n",
    "\n",
    "weather_col = 'weather'\n",
    "temp_col = 'temperature'\n",
    "\n",
    "\n",
    "# Fill missing values with the mode (most common value) of each column\n",
    "df = df.fillna(df.mode().iloc[0])\n",
    "# Step 1: Calculate acceptance ratio for each occupation\n",
    "acceptance_ratio = df.groupby('occupation')['Y'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Bin occupations based on acceptance ratio\n",
    "# Using quantiles as dynamic boundaries. Adjust according to your needs.\n",
    "bins = [\n",
    "    acceptance_ratio.min(),\n",
    "    acceptance_ratio.quantile(0.2),\n",
    "    acceptance_ratio.quantile(0.4),\n",
    "    acceptance_ratio.quantile(0.6),\n",
    "    acceptance_ratio.quantile(0.8),\n",
    "    acceptance_ratio.max()\n",
    "]\n",
    "\n",
    "bin_labels = ['low', 'medium_low', 'medium', 'medium_high', 'high']\n",
    "\n",
    "# Assign bin labels\n",
    "occupation_bins = pd.cut(acceptance_ratio, bins=bins, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "# Step 3: Map original occupation to occupation_class\n",
    "df['occupation'] = df['occupation'].map(occupation_bins.to_dict())\n",
    "\n",
    "\n",
    "# Define a function to combine the features into 'toCoupon'\n",
    "def combine_features(row):\n",
    "    if row['toCoupon_GEQ15min'] == 0:  # driving distance <= 15 min\n",
    "        return 0\n",
    "    elif row['toCoupon_GEQ25min'] == 0 :  # driving distance > 15 min and <= 25 min\n",
    "        return 1\n",
    "    else:  # driving distance > 25 min\n",
    "        return 2\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['toCoupon'] = df.apply(combine_features, axis=1)\n",
    "\n",
    "# Optionally, drop the original features\n",
    "df = df.drop(['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(data=df, x='toCoupon', hue='Y', palette='viridis', edgecolor='black', linewidth=0.7)\n",
    "\n",
    "\n",
    "# Feature Extraction for 'passenger_destination' from 'time' and 'destination'\n",
    "df['time_destination'] = df['time'].astype(str) + \"_\" + df['destination'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'marital_hasChildren' from 'maritalStatus' and 'has_children'\n",
    "df['marital_hasChildren'] = df['maritalStatus'].astype(str) + \"_\" + df['has_children'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'temperature_weather' from 'temperature' and 'weather'\n",
    "df['temperature_weather'] = df['temperature'].astype(str) + \"_\" + df['weather'].astype(str)\n",
    "\n",
    "\n",
    "df = df.drop(['time', 'destination', 'maritalStatus', 'has_children', 'temperature', 'weather'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Define order for the ordinal variables\n",
    "\n",
    "age_order = {'below21': 0, '21': 1, '26': 2, '31': 3, '36': 4, '41': 5, '46': 6, '50plus': 7}\n",
    "education_order = {'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2, 'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5}\n",
    "income_order = {'Less than $12500': 0, '$12500 - $24999': 1, '$25000 - $37499': 2, '$37500 - $49999': 3, '$50000 - $62499': 4, '$62500 - $74999': 5, '$75000 - $87499': 6, '$87500 - $99999': 7, '$100000 or More': 8}\n",
    "frequency_order = {'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}\n",
    "occupation_order= { 'medium_low':1, 'high':4, 'medium_high':3, 'low' :0,'medium':2}\n",
    "\n",
    "# Replace the values based on the order\n",
    "df['age'] = df['age'].replace(age_order)\n",
    "df['education'] = df['education'].replace(education_order)\n",
    "df['income'] = df['income'].replace(income_order)\n",
    "df['occupation']=df['occupation'].replace(occupation_order)\n",
    "\n",
    "# Encoding frequency-like features\n",
    "for col in ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']:\n",
    "    df[col] = df[col].replace(frequency_order)\n",
    "\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "onehot_cols = ['passanger', 'coupon', 'marital_hasChildren', 'temperature_weather', 'time_destination']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  \n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(df[onehot_cols]))\n",
    "\n",
    "# Reset indices to ensure alignment when concatenating\n",
    "encoded_cols.reset_index(drop=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Use appropriate column names for one-hot encoded columns\n",
    "encoded_cols.columns = encoder.get_feature_names_out(onehot_cols)\n",
    "\n",
    "# Concatenate the original dataframe and the one-hot encoded columns\n",
    "df = pd.concat([df, encoded_cols], axis=1)\n",
    "\n",
    "# Drop the original columns that were one-hot encoded\n",
    "df.drop(onehot_cols, axis=1, inplace=True)\n",
    "\n",
    "# 2. Binary Encoding\n",
    "df['expiration'] = df['expiration'].map({'2h': 0, '1d': 1})\n",
    "# Note: 'Y' and 'direction_same' are already binary, no encoding needed\n",
    "\n",
    "# 3. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])  # 0 for Female and 1 for Male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X_withY = df\n",
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]\n",
    "X = X.rename(columns={'coupon_Restaurant(<20)': 'coupon_Restaurant(20)'})\n",
    "\n",
    "#save X as csv\n",
    "#X.to_csv('X.csv', index=False)\n",
    "\n",
    "# Split data into 75% train+validation and 25% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize a k-fold cross-validator (e.g., 5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_standerdized= scaler.fit_transform(X_train_val)\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_test_standerdized= scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA to reduce dimensionality to 2 dimensions\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_standerdized)\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "X_test_pca = pca.transform(X_test_standerdized)\n",
    "df_test_pca = pd.DataFrame(data=X_test_pca, columns=['PC1', 'PC2'], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE to reduce dimensionality to 2 dimensions\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_standerdized)\n",
    "df_tsne = pd.DataFrame(data=X_tsne, columns=['t-SNE1', 't-SNE2'])\n",
    "# Perform t-SNE on the standardized test data\n",
    "X_test_tsne = tsne.fit_transform(X_test_standerdized)\n",
    "df_test_tsne = pd.DataFrame(data=X_test_tsne, columns=['t-SNE1', 't-SNE2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding K-Means' optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initialize a list to store the average silhouette scores for each k\n",
    "avg_silhouette_scores = []\n",
    "\n",
    "# Define a range of k values to try (e.g., 2 to 10)\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_pca)\n",
    "    k_means.fit(X_pca)\n",
    "    avg_silhouette_scores.append(sum(np.min(cdist(X_pca, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    #print('Found distortion for {} clusters'.format(k))\n",
    "\n",
    "X_line = [k_values[0], k_values[-1]]\n",
    "Y_line = [avg_silhouette_scores[0], avg_silhouette_scores[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(k_values, avg_silhouette_scores, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with KMeans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "df_kmeans_pca = pd.DataFrame({'Cluster_Kmeans_PCA': kmeans.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "df_kmeans_pca_test = pd.DataFrame({'Cluster_Kmeans_PCA': kmeans.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_kmeans_pca['Cluster_Kmeans_PCA']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_kmeans_pca_test['Cluster_Kmeans_PCA']], axis=1)\n",
    "\n",
    "y_pred = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=df_pca.join(df_kmeans_pca), hue=y_pred, palette='viridis', legend='full')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA with K-Means Clustering (k={})'.format(k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE with KMeans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_kmeans_tsne = pd.DataFrame({'Cluster_Kmeans_TSNE': kmeans.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "df_kmeans_tsne_test = pd.DataFrame({'Cluster_Kmeans_TSNE': kmeans.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_kmeans_tsne['Cluster_Kmeans_TSNE']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_kmeans_tsne_test['Cluster_Kmeans_TSNE']], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_Kmeans_TSNE', legend='full', palette='viridis', data=df_tsne.join(df_kmeans_tsne))\n",
    "plt.title('t-SNE with K-Means Clustering (k={})'.format(k))\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with Agglomerative method labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering\n",
    "cluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n",
    "df_agg_pca = pd.DataFrame({'Cluster_Agg_PCA': cluster.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "cluster_agg_pca_test = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n",
    "df_agg_pca_test = pd.DataFrame({'Cluster_Agg_PCA': cluster_agg_pca_test.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_agg_pca['Cluster_Agg_PCA']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_agg_pca_test['Cluster_Agg_PCA']], axis=1)\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_Agg_PCA', data=df_pca.join(df_agg_pca), palette='viridis')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tSNE with Agglomerative method labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_methods = ['ward', 'single', 'average', 'complete']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Iterate over linkage methods\n",
    "for i, linkage_method in enumerate(linkage_methods, 1):\n",
    "    # Perform hierarchical clustering on t-SNE for train set\n",
    "    cluster_agg_tsne = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='complete')\n",
    "    df_agg_tsne = pd.DataFrame({'Cluster_Agg_TSNE': cluster_agg_tsne.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "\n",
    "# Perform hierarchical clustering on t-SNE for test set\n",
    "    cluster_agg_tsne_test = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='complete')\n",
    "    df_agg_tsne_test = pd.DataFrame({'Cluster_Agg_TSNE': cluster_agg_tsne_test.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "\n",
    "# Concatenate hierarchical clustering clusters to original data for train and test sets\n",
    "    X_train_val_clustered = pd.concat([X_train_val, df_agg_tsne['Cluster_Agg_TSNE']], axis=1)\n",
    "    X_test_clustered = pd.concat([X_test, df_agg_tsne_test['Cluster_Agg_TSNE']], axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# Perform Affinity Propagation clustering on PCA for train set\n",
    "cluster_aff_pca = AffinityPropagation(random_state=42)\n",
    "df_aff_pca = pd.DataFrame({'Cluster_Aff_PCA': cluster_aff_pca.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Perform Affinity Propagation clustering on PCA for test set\n",
    "cluster_aff_pca_test = AffinityPropagation(random_state=42)\n",
    "df_aff_pca_test = pd.DataFrame({'Cluster_Aff_PCA': cluster_aff_pca_test.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "\n",
    "# Perform Affinity Propagation clustering on t-SNE for train set\n",
    "cluster_aff_tsne = AffinityPropagation(random_state=42)\n",
    "df_aff_tsne = pd.DataFrame({'Cluster_Aff_TSNE': cluster_aff_tsne.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "\n",
    "# Perform Affinity Propagation clustering on t-SNE for test set\n",
    "cluster_aff_tsne_test = AffinityPropagation(random_state=42)\n",
    "df_aff_tsne_test = pd.DataFrame({'Cluster_Aff_TSNE': cluster_aff_tsne_test.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "\n",
    "# Concatenate Affinity Propagation clusters to original data for train and test sets\n",
    "#X_train_val_clustered = pd.concat([X_train_val, df_aff_pca['Cluster_Aff_PCA']], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test, df_aff_pca_test['Cluster_Aff_PCA']], axis=1)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_aff_tsne['Cluster_Aff_TSNE']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_aff_tsne_test['Cluster_Aff_TSNE']], axis=1)\n",
    "# Check the number of features in X_train_val_clustered and X_test_clustered\n",
    "print(\"Number of features in X_train_val_clustered:\", X_train_val_clustered.shape[1])\n",
    "print(\"Number of features in X_test_clustered:\", X_test_clustered.shape[1])\n",
    "\n",
    "# Check for missing values in X_train_val_clustered and X_test_clustered\n",
    "print(\"Missing values in X_train_val_clustered:\", X_train_val_clustered.isnull().sum().sum())\n",
    "print(\"Missing values in X_test_clustered:\", X_test_clustered.isnull().sum().sum())\n",
    "\n",
    "# Visualize the clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot for PCA\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_Aff_PCA', data=df_aff_pca.join(df_pca), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('PCA with Affinity Propagation')\n",
    "\n",
    "# Plot for t-SNE\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_Aff_TSNE', data=df_aff_tsne.join(df_tsne), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('t-SNE with Affinity Propagation')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "# Perform Mean Shift clustering on PCA for train set\n",
    "cluster_ms_pca = MeanShift(bandwidth=2)\n",
    "df_ms_pca = pd.DataFrame({'Cluster_MS_PCA': cluster_ms_pca.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Perform Mean Shift clustering on PCA for test set\n",
    "cluster_ms_pca_test = MeanShift(bandwidth=2)\n",
    "df_ms_pca_test = pd.DataFrame({'Cluster_MS_PCA': cluster_ms_pca_test.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "\n",
    "# Perform Mean Shift clustering on t-SNE for train set\n",
    "cluster_ms_tsne = MeanShift(bandwidth=2)\n",
    "df_ms_tsne = pd.DataFrame({'Cluster_MS_TSNE': cluster_ms_tsne.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "\n",
    "# Perform Mean Shift clustering on t-SNE for test set\n",
    "cluster_ms_tsne_test = MeanShift(bandwidth=2)\n",
    "df_ms_tsne_test = pd.DataFrame({'Cluster_MS_TSNE': cluster_ms_tsne_test.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "\n",
    "# Concatenate Mean Shift clusters to original data for train and test sets\n",
    "#X_train_val_clustered = pd.concat([X_train_val, df_ms_pca['Cluster_MS_PCA']], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test, df_ms_pca_test['Cluster_MS_PCA']], axis=1)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_ms_tsne['Cluster_MS_TSNE']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_ms_tsne_test['Cluster_MS_TSNE']], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot for PCA\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_MS_PCA', data=df_ms_pca.join(df_pca), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('PCA with Mean Shift')\n",
    "\n",
    "# Plot for t-SNE\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_MS_TSNE', data=df_ms_tsne.join(df_tsne), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('t-SNE with Mean Shift')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Perform spectral clustering \n",
    "spectral_cluster = SpectralClustering(n_clusters=6, random_state=42)\n",
    "#df_sc_pca = pd.DataFrame({'Cluster_Spectral_PCA': spectral_cluster.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "#df_sc_pca_test = pd.DataFrame({'Cluster_Spectral_PCA': spectral_cluster.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "df_sc_tsne = pd.DataFrame({'Cluster_Spectral_TSNE': spectral_cluster.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "df_sc_tsne_test = pd.DataFrame({'Cluster_Spectral_TSNE': spectral_cluster.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "#X_train_val_clustered = pd.concat([X_train_val, df_sc_pca['Cluster_Spectral_PCA']], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test, df_sc_pca_test['Cluster_Spectral_PCA']], axis=1)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_sc_tsne['Cluster_Spectral_TSNE']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_sc_tsne_test['Cluster_Spectral_TSNE']], axis=1)\n",
    "\n",
    "# Visualize the clusters\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.scatterplot(x='PC1', y='PC2', hue='Cluster_Spectral_PCA', data=df_pca.join(df_sc_pca), palette='viridis')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Perform DBSCAN clustering on PCA for train set\n",
    "cluster_db_pca = DBSCAN(eps=2, min_samples=5)\n",
    "df_db_pca = pd.DataFrame({'Cluster_DB_PCA': cluster_db_pca.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Perform DBSCAN clustering on PCA for test set\n",
    "cluster_db_pca_test = DBSCAN(eps=2, min_samples=5)\n",
    "df_db_pca_test = pd.DataFrame({'Cluster_DB_PCA': cluster_db_pca_test.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "\n",
    "# Perform DBSCAN clustering on t-SNE for train set\n",
    "cluster_db_tsne = DBSCAN(eps=2, min_samples=5)\n",
    "df_db_tsne = pd.DataFrame({'Cluster_DB_TSNE': cluster_db_tsne.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "\n",
    "# Perform DBSCAN clustering on t-SNE for test set\n",
    "cluster_db_tsne_test = DBSCAN(eps=2, min_samples=5)\n",
    "df_db_tsne_test = pd.DataFrame({'Cluster_DB_TSNE': cluster_db_tsne_test.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "\n",
    "# Concatenate DBSCAN clusters to original data for train and test sets\n",
    "#X_train_val_clustered = pd.concat([X_train_val, df_db_pca['Cluster_DB_PCA']], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test, df_db_pca_test['Cluster_DB_PCA']], axis=1)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_db_tsne['Cluster_DB_TSNE']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_db_tsne_test['Cluster_DB_TSNE']], axis=1)\n",
    "\n",
    "# Visualize the clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot for PCA\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_DB_PCA', data=df_db_pca.join(df_pca), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('PCA with DBSCAN')\n",
    "\n",
    "# Plot for t-SNE\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_DB_TSNE', data=df_db_tsne.join(df_tsne), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('t-SNE with DBSCAN')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "# Perform OPTICS clustering on PCA for train set\n",
    "cluster_opt_pca = OPTICS(min_samples=5)\n",
    "df_opt_pca = pd.DataFrame({'Cluster_OPT_PCA': cluster_opt_pca.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Perform OPTICS clustering on PCA for test set\n",
    "cluster_opt_pca_test = OPTICS(min_samples=5)\n",
    "df_opt_pca_test = pd.DataFrame({'Cluster_OPT_PCA': cluster_opt_pca_test.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "\n",
    "# Perform OPTICS clustering on t-SNE for train set\n",
    "cluster_opt_tsne = OPTICS(min_samples=5)\n",
    "df_opt_tsne = pd.DataFrame({'Cluster_OPT_TSNE': cluster_opt_tsne.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "cluster_opt_tsne_test = OPTICS(min_samples=5)\n",
    "df_opt_tsne_test = pd.DataFrame({'Cluster_OPT_TSNE': cluster_opt_tsne_test.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "\n",
    "# Concatenate OPTICS clusters to original data for train and test sets\n",
    "#X_train_val_clustered = pd.concat([X_train_val, df_opt_pca['Cluster_OPT_PCA']], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test, df_opt_pca_test['Cluster_OPT_PCA']], axis=1)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_opt_tsne['Cluster_OPT_TSNE']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_opt_tsne_test['Cluster_OPT_TSNE']], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot for PCA\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_OPT_PCA', data=df_opt_pca.join(df_pca), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('PCA with OPTICS')\n",
    "\n",
    "# Plot for t-SNE\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_OPT_TSNE', data=df_opt_tsne.join(df_tsne), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('t-SNE with OPTICS')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "# Perform Birch clustering on PCA for train set\n",
    "cluster_birch_pca = Birch(n_clusters=6)\n",
    "df_birch_pca = pd.DataFrame({'Cluster_Birch_PCA': cluster_birch_pca.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Perform Birch clustering on PCA for test set\n",
    "cluster_birch_pca_test = Birch(n_clusters=6)\n",
    "df_birch_pca_test = pd.DataFrame({'Cluster_Birch_PCA': cluster_birch_pca_test.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "\n",
    "# Perform Birch clustering on t-SNE for train set\n",
    "cluster_birch_tsne = Birch(n_clusters=6)\n",
    "df_birch_tsne = pd.DataFrame({'Cluster_Birch_TSNE': cluster_birch_tsne.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "# Perform Birch clustering on t-SNE for test set\n",
    "cluster_birch_tsne_test = Birch(n_clusters=6)\n",
    "df_birch_tsne_test = pd.DataFrame({'Cluster_Birch_TSNE': cluster_birch_tsne_test.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "\n",
    "# Concatenate Birch clusters to original data for t-SNE\n",
    "#X_train_val_clustered = pd.concat([X_train_val, df_birch_tsne['Cluster_Birch_TSNE']], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test, df_birch_tsne_test['Cluster_Birch_TSNE']], axis=1)\n",
    "# Concatenate Birch clusters to original data\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_birch_pca['Cluster_Birch_PCA']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_birch_pca_test['Cluster_Birch_PCA']], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot for PCA\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_Birch_PCA', data=df_birch_pca.join(df_pca), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('PCA with Birch')\n",
    "\n",
    "# Plot for t-SNE\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_Birch_TSNE', data=df_birch_tsne.join(df_tsne), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('t-SNE with Birch')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "# Perform HDBSCAN clustering on PCA for train set\n",
    "cluster_pca = HDBSCAN(min_cluster_size=5)\n",
    "df_hd_pca = pd.DataFrame({'Cluster_PCA': cluster_pca.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Perform HDBSCAN clustering on PCA for test set\n",
    "df_hd_pca_test = pd.DataFrame({'Cluster_PCA': cluster_pca.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "\n",
    "# Perform HDBSCAN clustering on t-SNE for train set\n",
    "cluster_tsne = HDBSCAN(min_cluster_size=5)\n",
    "df_hd_tsne = pd.DataFrame({'Cluster_TSNE': cluster_tsne.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "df_hd_tsne_test = pd.DataFrame({'Cluster_TSNE': cluster_tsne.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "# Concatenate clusters to original data for train and test sets\n",
    "#X_train_val_clustered = pd.concat([X_train_val, df_hd_pca['Cluster_PCA']], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test, df_hd_pca_test['Cluster_PCA']], axis=1)\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_hd_tsne['Cluster_TSNE']], axis=1)\n",
    "X_test_clustered = pd.concat([X_test, df_hd_tsne_test['Cluster_TSNE']], axis=1)\n",
    "\n",
    "\n",
    "# Visualize the clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot for PCA\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_PCA', data=df_hd_pca.join(df_pca), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('PCA with HDBSCAN')\n",
    "\n",
    "# Plot for t-SNE\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_TSNE', data=df_hd_tsne.join(df_tsne), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('t-SNE with HDBSCAN')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Perform Gaussian Mixture Model clustering on PCA for train set\n",
    "cluster_pca_gmm = GaussianMixture(n_components=6, random_state=42)\n",
    "df_gmm_pca = pd.DataFrame({'Cluster_PCA_GMM': cluster_pca_gmm.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Perform Gaussian Mixture Model clustering on PCA for test set\n",
    "cluster_pca_gmm_test = GaussianMixture(n_components=6, random_state=42)\n",
    "df_gmm_pca_test = pd.DataFrame({'Cluster_PCA_GMM': cluster_pca_gmm_test.fit_predict(X_test_pca)}, index=X_test.index)\n",
    "\n",
    "# Perform Gaussian Mixture Model clustering on t-SNE for train set\n",
    "cluster_tsne_gmm = GaussianMixture(n_components=6, random_state=42)\n",
    "df_gmm_tsne = pd.DataFrame({'Cluster_TSNE_GMM': cluster_tsne_gmm.fit_predict(X_tsne)}, index=X_train_val.index)\n",
    "cluster_tsne_gmm_test = GaussianMixture(n_components=6, random_state=42)\n",
    "df_gmm_tsne_test = pd.DataFrame({'Cluster_TSNE_GMM': cluster_tsne_gmm_test.fit_predict(X_test_tsne)}, index=X_test.index)\n",
    "# Concatenate GMM clusters to original data\n",
    "X_train_val_clustered= pd.concat([X_train_val, df_gmm_pca['Cluster_PCA_GMM']], axis=1)\n",
    "X_test_clustered= pd.concat([X_test, df_gmm_pca_test['Cluster_PCA_GMM']], axis=1)\n",
    "#X_train_val_clustered= pd.concat([X_train_val, df_gmm_tsne['Cluster_TSNE_GMM']], axis=1)\n",
    "#X_test_clustered= pd.concat([X_test, df_gmm_tsne_test['Cluster_TSNE_GMM']], axis=1)\n",
    "# Visualize the clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plot for PCA\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster_PCA_GMM', data=df_gmm_pca.join(df_pca), palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('PCA with Gaussian Mixture Model')\n",
    "\n",
    "# Plot for t-SNE\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster_TSNE_GMM', data=df_gmm_tsne.join(df_tsne), palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('t-SNE with Gaussian Mixture Model')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost \n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Best parameters for each model\n",
    "catboost_params = {'depth': 8, 'iterations': 1000, 'l2_leaf_reg': 1, 'learning_rate': 0.05}\n",
    "bagging_params = {'bootstrap': False, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
    "gradient_boosting_params = {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "hist_gradient_boosting_params = {'learning_rate': 0.1, 'max_depth': 15, 'max_iter': 200, 'min_samples_leaf': 10}\n",
    "random_forest_params = {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
    "xgboost_params = {'learning_rate': 0.22000000000000003, 'max_depth': 6, 'n_estimators': 207, 'objective': 'binary:logistic', 'use_label_encoder': False}  # Added XGBoost parameters\n",
    "ada_params =  {'learning_rate': 1, 'n_estimators': 150}\n",
    "\n",
    "# Base classifiers\n",
    "catboost = CatBoostClassifier(**catboost_params, random_seed=42, verbose=0)\n",
    "bagging = BaggingClassifier(**bagging_params, random_state=42)\n",
    "gradient_boosting = GradientBoostingClassifier(**gradient_boosting_params, random_state=42)\n",
    "hist_gradient_boosting = HistGradientBoostingClassifier(**hist_gradient_boosting_params, random_state=42)\n",
    "random_forest = RandomForestClassifier(**random_forest_params, random_state=42)\n",
    "xgboost = XGBClassifier(**xgboost_params)  \n",
    "\n",
    "ada = AdaBoostClassifier(**ada_params,random_state=42)\n",
    "\n",
    "# Updated estimators list\n",
    "estimators = [\n",
    "    ('catboost', catboost),\n",
    "    ('bagging', bagging),\n",
    "    ('gradient_boosting', gradient_boosting),\n",
    "    ('hist_gradient_boosting', hist_gradient_boosting),\n",
    "    ('random_forest', random_forest),\n",
    "    ('xgboost', xgboost),\n",
    "    ('ada',ada)\n",
    "]\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=None, cv=kf, n_jobs=-1)\n",
    "\n",
    "# Assuming you've already defined your training and test sets as X_train_val, y_train_val and X_test, y_test respectively\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train_val_clustered, y_train_val)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_preds_stacking = stacking_clf.predict(X_test_clustered)\n",
    "\n",
    "# Evaluating the Stacking model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Stacking Classifier test Accuracy:\", accuracy_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# Precision\n",
    "print(\"Stacking Classifier test Precision:\", precision_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# Recall\n",
    "print(\"Stacking Classifier test Recall:\", recall_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_stacking = stacking_clf.predict_proba(X_test_clustered)[:, 1]\n",
    "print(\"Stacking Classifier ROC-AUC:\", roc_auc_score(y_test, y_test_prob_stacking))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Stacking Classifier Log Loss:\", log_loss(y_test, y_test_prob_stacking))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_stacking, tpr_stacking, _ = roc_curve(y_test, y_test_prob_stacking)\n",
    "print(\"Stacking Classifier AUC Score:\", auc(fpr_stacking, tpr_stacking))\n",
    "print(\"Stacking Classifier report\", classification_report(y_test,y_test_preds_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "cluster_label_name = 'Cluster_Kmeans_PCA'\n",
    "cluster_method = df_kmeans_pca\n",
    "\n",
    "df_metrics = pd.concat([X_train_val, cluster_method[cluster_label_name]], axis=1)\n",
    "\n",
    "X_train_val_clustered = pd.concat([df_metrics, y_train_val], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test_clustered, y_test], axis=1)\n",
    "\n",
    "cluster_labels = X_train_val_clustered[cluster_label_name].unique()\n",
    "y_labels = X_train_val_clustered['Y'].unique()\n",
    "\n",
    "adjusted_rand_score = metrics.adjusted_rand_score(X_train_val_clustered['Y'], X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "adjusted_mutual_info_score = metrics.adjusted_mutual_info_score(X_train_val_clustered['Y'], X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "homogeneity_score = metrics.homogeneity_score(X_train_val_clustered['Y'], X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "completeness_score = metrics.completeness_score(X_train_val_clustered['Y'], X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "v_measure_score = metrics.v_measure_score(X_train_val_clustered['Y'], X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "fowlkes_mallows_score = metrics.fowlkes_mallows_score(X_train_val_clustered['Y'], X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "print(f\"Adjusted Rand Score: {adjusted_rand_score}\")\n",
    "print(f\"Adjusted Mutual Information Score: {adjusted_mutual_info_score}\")\n",
    "print(f\"Homogeneity Score: {homogeneity_score}\")\n",
    "print(f\"Completeness Score: {completeness_score}\")\n",
    "print(f\"V-Measure Score: {v_measure_score}\")\n",
    "print(f\"Fowlkes Mallows Score: {fowlkes_mallows_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "cluster_label_name = 'Cluster_Kmeans_PCA'\n",
    "cluster_method = df_kmeans_pca\n",
    "\n",
    "df_metrics = pd.concat([X_train_val, cluster_method[cluster_label_name]], axis=1)\n",
    "\n",
    "X_train_val_clustered = pd.concat([df_metrics, y_train_val], axis=1)\n",
    "#X_test_clustered = pd.concat([X_test_clustered, y_test], axis=1)\n",
    "\n",
    "cluster_labels = X_train_val_clustered[cluster_label_name].unique()\n",
    "y_labels = X_train_val_clustered['Y'].unique()\n",
    "\n",
    "silhouette_score = metrics.silhouette_score(X_train_val_clustered.drop([cluster_label_name, 'Y'], axis=1), X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "davies_bouldin_score = metrics.davies_bouldin_score(X_train_val_clustered.drop([cluster_label_name, 'Y'], axis=1), X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "calinski_harabasz_score = metrics.calinski_harabasz_score(X_train_val_clustered.drop([cluster_label_name, 'Y'], axis=1), X_train_val_clustered[cluster_label_name])\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score}\")\n",
    "print(f\"Calinski Harabasz Score: {calinski_harabasz_score}\")\n",
    "print(f\"Davies Bouldin Score: {davies_bouldin_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_train = X_train_val_clustered.corr()\n",
    "\n",
    "plt.figure(figsize=(24, 22))\n",
    "sns.heatmap(correlation_matrix_train, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix for Train Set')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
