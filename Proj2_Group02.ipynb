{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import researchpy as rp\n",
    "from scipy import stats\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score,precision_score, recall_score, roc_auc_score, log_loss, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier, GradientBoostingClassifier,BaggingClassifier,StackingClassifier\n",
    "\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file.\n",
    "csv_file = 'in-vehicle-coupon-recommendation.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame.\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.drop_duplicates()\n",
    "print(\"Shape of dataset after removing duplicates:\",df.shape)\n",
    "\n",
    "# Drop the 'car' , 'toCoupon_GEQ5min' and 'direction_opp' columns\n",
    "df = df.drop(['car', 'toCoupon_GEQ5min','direction_opp'], axis=1)\n",
    "\n",
    "weather_col = 'weather'\n",
    "temp_col = 'temperature'\n",
    "\n",
    "\n",
    "# Fill missing values with the mode (most common value) of each column\n",
    "df = df.fillna(df.mode().iloc[0])\n",
    "# Step 1: Calculate acceptance ratio for each occupation\n",
    "acceptance_ratio = df.groupby('occupation')['Y'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Bin occupations based on acceptance ratio\n",
    "# Using quantiles as dynamic boundaries. Adjust according to your needs.\n",
    "bins = [\n",
    "    acceptance_ratio.min(),\n",
    "    acceptance_ratio.quantile(0.2),\n",
    "    acceptance_ratio.quantile(0.4),\n",
    "    acceptance_ratio.quantile(0.6),\n",
    "    acceptance_ratio.quantile(0.8),\n",
    "    acceptance_ratio.max()\n",
    "]\n",
    "\n",
    "bin_labels = ['low', 'medium_low', 'medium', 'medium_high', 'high']\n",
    "\n",
    "# Assign bin labels\n",
    "occupation_bins = pd.cut(acceptance_ratio, bins=bins, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "# Step 3: Map original occupation to occupation_class\n",
    "df['occupation'] = df['occupation'].map(occupation_bins.to_dict())\n",
    "\n",
    "\n",
    "# Define a function to combine the features into 'toCoupon'\n",
    "def combine_features(row):\n",
    "    if row['toCoupon_GEQ15min'] == 0:  # driving distance <= 15 min\n",
    "        return 0\n",
    "    elif row['toCoupon_GEQ25min'] == 0 :  # driving distance > 15 min and <= 25 min\n",
    "        return 1\n",
    "    else:  # driving distance > 25 min\n",
    "        return 2\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['toCoupon'] = df.apply(combine_features, axis=1)\n",
    "\n",
    "# Optionally, drop the original features\n",
    "df = df.drop(['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(data=df, x='toCoupon', hue='Y', palette='viridis', edgecolor='black', linewidth=0.7)\n",
    "\n",
    "\n",
    "# Feature Extraction for 'passenger_destination' from 'time' and 'destination'\n",
    "df['time_destination'] = df['time'].astype(str) + \"_\" + df['destination'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'marital_hasChildren' from 'maritalStatus' and 'has_children'\n",
    "df['marital_hasChildren'] = df['maritalStatus'].astype(str) + \"_\" + df['has_children'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'temperature_weather' from 'temperature' and 'weather'\n",
    "df['temperature_weather'] = df['temperature'].astype(str) + \"_\" + df['weather'].astype(str)\n",
    "\n",
    "\n",
    "df = df.drop(['time', 'destination', 'maritalStatus', 'has_children', 'temperature', 'weather'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Define order for the ordinal variables\n",
    "\n",
    "age_order = {'below21': 0, '21': 1, '26': 2, '31': 3, '36': 4, '41': 5, '46': 6, '50plus': 7}\n",
    "education_order = {'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2, 'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5}\n",
    "income_order = {'Less than $12500': 0, '$12500 - $24999': 1, '$25000 - $37499': 2, '$37500 - $49999': 3, '$50000 - $62499': 4, '$62500 - $74999': 5, '$75000 - $87499': 6, '$87500 - $99999': 7, '$100000 or More': 8}\n",
    "frequency_order = {'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}\n",
    "occupation_order= { 'medium_low':1, 'high':4, 'medium_high':3, 'low' :0,'medium':2}\n",
    "\n",
    "# Replace the values based on the order\n",
    "df['age'] = df['age'].replace(age_order)\n",
    "df['education'] = df['education'].replace(education_order)\n",
    "df['income'] = df['income'].replace(income_order)\n",
    "df['occupation']=df['occupation'].replace(occupation_order)\n",
    "\n",
    "# Encoding frequency-like features\n",
    "for col in ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']:\n",
    "    df[col] = df[col].replace(frequency_order)\n",
    "\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "onehot_cols = ['passanger', 'coupon', 'marital_hasChildren', 'temperature_weather', 'time_destination']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  \n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(df[onehot_cols]))\n",
    "\n",
    "# Reset indices to ensure alignment when concatenating\n",
    "encoded_cols.reset_index(drop=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Use appropriate column names for one-hot encoded columns\n",
    "encoded_cols.columns = encoder.get_feature_names_out(onehot_cols)\n",
    "\n",
    "# Concatenate the original dataframe and the one-hot encoded columns\n",
    "df = pd.concat([df, encoded_cols], axis=1)\n",
    "\n",
    "# Drop the original columns that were one-hot encoded\n",
    "df.drop(onehot_cols, axis=1, inplace=True)\n",
    "\n",
    "# 2. Binary Encoding\n",
    "df['expiration'] = df['expiration'].map({'2h': 0, '1d': 1})\n",
    "# Note: 'Y' and 'direction_same' are already binary, no encoding needed\n",
    "\n",
    "# 3. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])  # 0 for Female and 1 for Male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]\n",
    "X = X.rename(columns={'coupon_Restaurant(<20)': 'coupon_Restaurant(20)'})\n",
    "\n",
    "#save X as csv\n",
    "#X.to_csv('X.csv', index=False)\n",
    "\n",
    "# Split data into 75% train+validation and 25% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize a k-fold cross-validator (e.g., 5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_standerdized= scaler.fit_transform(X_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA to reduce dimensionality to 2 dimensions\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_standerdized)\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE to reduce dimensionality to 2 dimensions\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_standerdized)\n",
    "df_tsne = pd.DataFrame(data=X_tsne, columns=['t-SNE1', 't-SNE2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding K-Means' optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initialize a list to store the average silhouette scores for each k\n",
    "avg_silhouette_scores = []\n",
    "\n",
    "# Define a range of k values to try (e.g., 2 to 10)\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_pca)\n",
    "    k_means.fit(X_pca)\n",
    "    avg_silhouette_scores.append(sum(np.min(cdist(X_pca, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    #print('Found distortion for {} clusters'.format(k))\n",
    "\n",
    "X_line = [k_values[0], k_values[-1]]\n",
    "Y_line = [avg_silhouette_scores[0], avg_silhouette_scores[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(k_values, avg_silhouette_scores, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with KMeans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=df_pca, hue=y_pred, palette='viridis', legend='full')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('K-Means Clustering (k=6)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE with KMeans labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_tsne = pd.DataFrame({'t-SNE1': X_tsne[:, 0], 't-SNE2': X_tsne[:, 1], 'Cluster': y_pred})\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(20, l=.4, s=.9)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='Cluster', legend='full', palette='viridis', data=df_tsne)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "plt.savefig(\"improved_cluster_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering\n",
    "cluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n",
    "df_train_clusters = pd.DataFrame({'Cluster': cluster.fit_predict(X_pca)}, index=X_train_val.index)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=df_train_clusters.join(df_pca), palette='viridis')\n",
    "plt.title('Hierarchical Clustering - Train Set')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
