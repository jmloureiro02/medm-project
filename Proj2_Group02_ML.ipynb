{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import researchpy as rp\n",
    "from scipy import stats\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score,precision_score, recall_score, roc_auc_score, log_loss, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier, GradientBoostingClassifier,BaggingClassifier,StackingClassifier\n",
    "\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file.\n",
    "csv_file = 'in-vehicle-coupon-recommendation.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame.\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.drop_duplicates()\n",
    "print(\"Shape of dataset after removing duplicates:\",df.shape)\n",
    "\n",
    "# Drop the 'car' , 'toCoupon_GEQ5min' and 'direction_opp' columns\n",
    "df = df.drop(['car', 'toCoupon_GEQ5min','direction_opp'], axis=1)\n",
    "\n",
    "weather_col = 'weather'\n",
    "temp_col = 'temperature'\n",
    "\n",
    "\n",
    "# Fill missing values with the mode (most common value) of each column\n",
    "df = df.fillna(df.mode().iloc[0])\n",
    "# Step 1: Calculate acceptance ratio for each occupation\n",
    "acceptance_ratio = df.groupby('occupation')['Y'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Bin occupations based on acceptance ratio\n",
    "# Using quantiles as dynamic boundaries. Adjust according to your needs.\n",
    "bins = [\n",
    "    acceptance_ratio.min(),\n",
    "    acceptance_ratio.quantile(0.2),\n",
    "    acceptance_ratio.quantile(0.4),\n",
    "    acceptance_ratio.quantile(0.6),\n",
    "    acceptance_ratio.quantile(0.8),\n",
    "    acceptance_ratio.max()\n",
    "]\n",
    "\n",
    "bin_labels = ['low', 'medium_low', 'medium', 'medium_high', 'high']\n",
    "\n",
    "# Assign bin labels\n",
    "occupation_bins = pd.cut(acceptance_ratio, bins=bins, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "# Step 3: Map original occupation to occupation_class\n",
    "df['occupation'] = df['occupation'].map(occupation_bins.to_dict())\n",
    "\n",
    "\n",
    "# Define a function to combine the features into 'toCoupon'\n",
    "def combine_features(row):\n",
    "    if row['toCoupon_GEQ15min'] == 0:  # driving distance <= 15 min\n",
    "        return 0\n",
    "    elif row['toCoupon_GEQ25min'] == 0 :  # driving distance > 15 min and <= 25 min\n",
    "        return 1\n",
    "    else:  # driving distance > 25 min\n",
    "        return 2\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['toCoupon'] = df.apply(combine_features, axis=1)\n",
    "\n",
    "# Optionally, drop the original features\n",
    "df = df.drop(['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(data=df, x='toCoupon', hue='Y', palette='viridis', edgecolor='black', linewidth=0.7)\n",
    "\n",
    "\n",
    "# Feature Extraction for 'passenger_destination' from 'time' and 'destination'\n",
    "df['time_destination'] = df['time'].astype(str) + \"_\" + df['destination'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'marital_hasChildren' from 'maritalStatus' and 'has_children'\n",
    "df['marital_hasChildren'] = df['maritalStatus'].astype(str) + \"_\" + df['has_children'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'temperature_weather' from 'temperature' and 'weather'\n",
    "df['temperature_weather'] = df['temperature'].astype(str) + \"_\" + df['weather'].astype(str)\n",
    "\n",
    "\n",
    "df = df.drop(['time', 'destination', 'maritalStatus', 'has_children', 'temperature', 'weather'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Define order for the ordinal variables\n",
    "\n",
    "age_order = {'below21': 0, '21': 1, '26': 2, '31': 3, '36': 4, '41': 5, '46': 6, '50plus': 7}\n",
    "education_order = {'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2, 'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5}\n",
    "income_order = {'Less than $12500': 0, '$12500 - $24999': 1, '$25000 - $37499': 2, '$37500 - $49999': 3, '$50000 - $62499': 4, '$62500 - $74999': 5, '$75000 - $87499': 6, '$87500 - $99999': 7, '$100000 or More': 8}\n",
    "frequency_order = {'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}\n",
    "occupation_order= { 'medium_low':1, 'high':4, 'medium_high':3, 'low' :0,'medium':2}\n",
    "\n",
    "# Replace the values based on the order\n",
    "df['age'] = df['age'].replace(age_order)\n",
    "df['education'] = df['education'].replace(education_order)\n",
    "df['income'] = df['income'].replace(income_order)\n",
    "df['occupation']=df['occupation'].replace(occupation_order)\n",
    "\n",
    "# Encoding frequency-like features\n",
    "for col in ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']:\n",
    "    df[col] = df[col].replace(frequency_order)\n",
    "\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "onehot_cols = ['passanger', 'coupon', 'marital_hasChildren', 'temperature_weather', 'time_destination']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  \n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(df[onehot_cols]))\n",
    "\n",
    "# Reset indices to ensure alignment when concatenating\n",
    "encoded_cols.reset_index(drop=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Use appropriate column names for one-hot encoded columns\n",
    "encoded_cols.columns = encoder.get_feature_names_out(onehot_cols)\n",
    "\n",
    "# Concatenate the original dataframe and the one-hot encoded columns\n",
    "df = pd.concat([df, encoded_cols], axis=1)\n",
    "\n",
    "# Drop the original columns that were one-hot encoded\n",
    "df.drop(onehot_cols, axis=1, inplace=True)\n",
    "\n",
    "# 2. Binary Encoding\n",
    "df['expiration'] = df['expiration'].map({'2h': 0, '1d': 1})\n",
    "# Note: 'Y' and 'direction_same' are already binary, no encoding needed\n",
    "\n",
    "# 3. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])  # 0 for Female and 1 for Male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]\n",
    "X = X.rename(columns={'coupon_Restaurant(<20)': 'coupon_Restaurant(20)'})\n",
    "\n",
    "\n",
    "# Split data into 75% train+validation and 25% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize a k-fold cross-validator (e.g., 5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data for clustering\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X_train_val)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "cluster = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')\n",
    "df_train_clusters = pd.DataFrame({'Cluster': cluster.fit_predict(X_standardized)}, index=X_train_val.index)\n",
    "\n",
    "# Print the number of features in X_train_val\n",
    "print(\"Number of features in X_train_val:\", X_train_val.shape[1])\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_standardized)\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'], index=X_train_val.index)\n",
    "\n",
    "# Concatenate X_train_val and the Cluster column\n",
    "X_train_val_clustered = pd.concat([X_train_val, df_train_clusters['Cluster']], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Print the number of features in X_train_val_clustered\n",
    "print(\"Number of features in X_train_val_clustered:\", X_train_val_clustered.shape[1])\n",
    "\n",
    "# Print the columns of df_train_clusters (should only contain 'Cluster')\n",
    "print(\"Columns of df_train_clusters:\", df_train_clusters.columns)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=df_train_clusters.join(df_pca), palette='viridis')\n",
    "plt.title('Hierarchical Clustering - Train Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data for clustering\n",
    "scaler = StandardScaler()\n",
    "X_test_standardized = scaler.fit_transform(X_test)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "cluster = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')\n",
    "df_test_clusters = pd.DataFrame({'Cluster': cluster.fit_predict(X_test_standardized)}, index=X_test.index)\n",
    "\n",
    "# Print the number of features in X_test\n",
    "print(\"Number of features in X_test:\", X_test.shape[1])\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "principal_components_test = pca.fit_transform(X_test_standardized)\n",
    "df_pca_test = pd.DataFrame(data=principal_components_test, columns=['PC1', 'PC2'], index=X_test.index)\n",
    "\n",
    "# Concatenate X_test and the Cluster column\n",
    "X_test_clustered = pd.concat([X_test, df_test_clusters['Cluster']], axis=1)\n",
    "\n",
    "# Print the number of features in X_test_clustered\n",
    "print(\"Number of features in X_test_clustered:\", X_test_clustered.shape[1])\n",
    "\n",
    "# Print the columns of df_test_clusters (should only contain 'Cluster')\n",
    "print(\"Columns of df_test_clusters:\", df_test_clusters.columns)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=df_test_clusters.join(df_pca_test), palette='viridis')\n",
    "plt.title('Hierarchical Clustering - Test Set')  # Update the title to reflect the test set\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs']\n",
    "    },\n",
    "    {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the base Logistic Regression model\n",
    "clf_logistic_regression = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(clf_logistic_regression, param_grid, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_test_preds = best_clf.predict(X_test)\n",
    "# Evaluating the model\n",
    "\n",
    "# Accuracy\n",
    "print(\" clf_logistic_regression Validation Accuracy:\", accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "# Precision\n",
    "print(\"clf_logistic_regression Validation Precision:\", precision_score(y_test, y_test_preds))\n",
    "\n",
    "# Recall\n",
    "print(\"clf_logistic_regression Validation Recall:\", recall_score(y_test, y_test_preds))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob = best_clf.predict_proba(X_test)[:, 1]\n",
    "print(\"clf_logistic_regression ROC-AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "\n",
    "# Log Loss\n",
    "print(\"clf_logistic_regression Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "print(\"clf_logistic_regression AUC Score:\", auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': list(range(1, 21)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Initialize KNN classifier\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV for KNN\n",
    "grid_search_knn = GridSearchCV(clf_knn, knn_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_knn.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for KNN\n",
    "print(f\"KNN Best Parameters: {grid_search_knn.best_params_}\")\n",
    "print(f\"KNN Best Cross-Validation Accuracy: {grid_search_knn.best_score_}\")\n",
    "\n",
    "# Use the best KNN model to make predictions on the test set\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "y_test_preds_knn = best_knn.predict(X_test)\n",
    "\n",
    "# Evaluating the KNN model\n",
    "\n",
    "# Accuracy\n",
    "print(\"KNN Validation Accuracy:\", accuracy_score(y_test, y_test_preds_knn))\n",
    "\n",
    "# Precision\n",
    "print(\"KNN Validation Precision:\", precision_score(y_test, y_test_preds_knn))\n",
    "\n",
    "# Recall\n",
    "print(\"KNN Validation Recall:\", recall_score(y_test, y_test_preds_knn))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_knn = best_knn.predict_proba(X_test)[:, 1]\n",
    "print(\"KNN ROC-AUC:\", roc_auc_score(y_test, y_test_prob_knn))\n",
    "\n",
    "# Log Loss\n",
    "print(\"KNN Log Loss:\", log_loss(y_test, y_test_prob_knn))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_test_prob_knn)\n",
    "print(\"KNN AUC Score:\", auc(fpr_knn, tpr_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for Decision Tree\n",
    "dt_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize Decision Tree classifier\n",
    "clf_dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for Decision Tree\n",
    "grid_search_dt = GridSearchCV(clf_dt, dt_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_dt.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for Decision Tree\n",
    "print(f\"Decision Tree Best Parameters: {grid_search_dt.best_params_}\")\n",
    "print(f\"Decision Tree Best Cross-Validation Accuracy: {grid_search_dt.best_score_}\")\n",
    "\n",
    "# Use the best Decision Tree model to make predictions on the test set\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "y_test_preds_dt = best_dt.predict(X_test)\n",
    "\n",
    "# Evaluating the Decision Tree model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Decision Tree Validation Accuracy:\", accuracy_score(y_test, y_test_preds_dt))\n",
    "\n",
    "# Precision\n",
    "print(\"Decision Tree Validation Precision:\", precision_score(y_test, y_test_preds_dt))\n",
    "\n",
    "# Recall\n",
    "print(\"Decision Tree Validation Recall:\", recall_score(y_test, y_test_preds_dt))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_dt = best_dt.predict_proba(X_test)[:, 1]\n",
    "print(\"Decision Tree ROC-AUC:\", roc_auc_score(y_test, y_test_prob_dt))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Decision Tree Log Loss:\", log_loss(y_test, y_test_prob_dt))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_test_prob_dt)\n",
    "print(\"Decision Tree AUC Score:\", auc(fpr_dt, tpr_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for SVC\n",
    "svc_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize SVC classifier with RBF kernel\n",
    "clf_svc = SVC(kernel='rbf', probability=True, random_state=42) # probability=True to ensure we can use predict_proba later\n",
    "\n",
    "# Initialize GridSearchCV for SVC\n",
    "grid_search_svc = GridSearchCV(clf_svc, svc_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_svc.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for SVC\n",
    "print(f\"SVC Best Parameters: {grid_search_svc.best_params_}\")\n",
    "print(f\"SVC Best Cross-Validation Accuracy: {grid_search_svc.best_score_}\")\n",
    "\n",
    "# Use the best SVC model to make predictions on the test set\n",
    "best_svc = grid_search_svc.best_estimator_\n",
    "y_test_preds_svc = best_svc.predict(X_test)\n",
    "\n",
    "# Evaluating the SVC model\n",
    "\n",
    "# Accuracy\n",
    "print(\"SVC Validation Accuracy:\", accuracy_score(y_test, y_test_preds_svc))\n",
    "\n",
    "# Precision\n",
    "print(\"SVC Validation Precision:\", precision_score(y_test, y_test_preds_svc))\n",
    "\n",
    "# Recall\n",
    "print(\"SVC Validation Recall:\", recall_score(y_test, y_test_preds_svc))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_svc = best_svc.predict_proba(X_test)[:, 1]\n",
    "print(\"SVC ROC-AUC:\", roc_auc_score(y_test, y_test_prob_svc))\n",
    "\n",
    "# Log Loss\n",
    "print(\"SVC Log Loss:\", log_loss(y_test, y_test_prob_svc))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_svc, tpr_svc, _ = roc_curve(y_test, y_test_prob_svc)\n",
    "print(\"SVC AUC Score:\", auc(fpr_svc, tpr_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for LinearSVC\n",
    "linear_svc_params = {\n",
    "    'estimator__C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Initialize LinearSVC classifier\n",
    "clf_linear_svc = LinearSVC(max_iter=10000, random_state=42)  # Increased max_iter for convergence\n",
    "\n",
    "# For probability outputs, wrap LinearSVC within CalibratedClassifierCV\n",
    "clf_calibrated = CalibratedClassifierCV(clf_linear_svc, method='sigmoid', cv=5)\n",
    "\n",
    "# Initialize GridSearchCV for LinearSVC\n",
    "grid_search_linear_svc = GridSearchCV(clf_calibrated, linear_svc_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_linear_svc.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for LinearSVC\n",
    "print(f\"LinearSVC Best Parameters: {grid_search_linear_svc.best_params_}\")\n",
    "print(f\"LinearSVC Best Cross-Validation Accuracy: {grid_search_linear_svc.best_score_}\")\n",
    "\n",
    "# Use the best LinearSVC model to make predictions on the test set\n",
    "best_linear_svc = grid_search_linear_svc.best_estimator_\n",
    "y_test_preds_linear_svc = best_linear_svc.predict(X_test)\n",
    "\n",
    "# Evaluating the LinearSVC model\n",
    "\n",
    "# Accuracy\n",
    "print(\"LinearSVC Validation Accuracy:\", accuracy_score(y_test, y_test_preds_linear_svc))\n",
    "\n",
    "# Precision\n",
    "print(\"LinearSVC Validation Precision:\", precision_score(y_test, y_test_preds_linear_svc))\n",
    "\n",
    "# Recall\n",
    "print(\"LinearSVC Validation Recall:\", recall_score(y_test, y_test_preds_linear_svc))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_linear_svc = best_linear_svc.predict_proba(X_test)[:, 1]\n",
    "print(\"LinearSVC ROC-AUC:\", roc_auc_score(y_test, y_test_prob_linear_svc))\n",
    "\n",
    "# Log Loss\n",
    "print(\"LinearSVC Log Loss:\", log_loss(y_test, y_test_prob_linear_svc))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_linear_svc, tpr_linear_svc, _ = roc_curve(y_test, y_test_prob_linear_svc)\n",
    "print(\"LinearSVC AUC Score:\", auc(fpr_linear_svc, tpr_linear_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for RandomForestClassifier\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for RandomForest\n",
    "grid_search_rf = GridSearchCV(clf_rf, rf_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for RandomForest\n",
    "print(f\"Random Forest Best Parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Random Forest Best Cross-Validation Accuracy: {grid_search_rf.best_score_}\")\n",
    "\n",
    "# Use the best RandomForest model to make predictions on the test set\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_test_preds_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluating the RandomForest model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Random Forest Validation Accuracy:\", accuracy_score(y_test, y_test_preds_rf))\n",
    "\n",
    "# Precision\n",
    "print(\"Random Forest Validation Precision:\", precision_score(y_test, y_test_preds_rf))\n",
    "\n",
    "# Recall\n",
    "print(\"Random Forest Validation Recall:\", recall_score(y_test, y_test_preds_rf))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "print(\"Random Forest ROC-AUC:\", roc_auc_score(y_test, y_test_prob_rf))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Random Forest Log Loss:\", log_loss(y_test, y_test_prob_rf))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_prob_rf)\n",
    "print(\"Random Forest AUC Score:\", auc(fpr_rf, tpr_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra trees clssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for ExtraTreesClassifier\n",
    "et_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize ExtraTreesClassifier\n",
    "clf_et = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for ExtraTrees\n",
    "grid_search_et = GridSearchCV(clf_et, et_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_et.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for ExtraTrees\n",
    "print(f\"Extra Trees Best Parameters: {grid_search_et.best_params_}\")\n",
    "print(f\"Extra Trees Best Cross-Validation Accuracy: {grid_search_et.best_score_}\")\n",
    "\n",
    "# Use the best ExtraTrees model to make predictions on the test set\n",
    "best_et = grid_search_et.best_estimator_\n",
    "y_test_preds_et = best_et.predict(X_test)\n",
    "\n",
    "# Evaluating the ExtraTrees model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Extra Trees Validation Accuracy:\", accuracy_score(y_test, y_test_preds_et))\n",
    "\n",
    "# Precision\n",
    "print(\"Extra Trees Validation Precision:\", precision_score(y_test, y_test_preds_et))\n",
    "\n",
    "# Recall\n",
    "print(\"Extra Trees Validation Recall:\", recall_score(y_test, y_test_preds_et))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_et = best_et.predict_proba(X_test)[:, 1]\n",
    "print(\"Extra Trees ROC-AUC:\", roc_auc_score(y_test, y_test_prob_et))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Extra Trees Log Loss:\", log_loss(y_test, y_test_prob_et))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_et, tpr_et, _ = roc_curve(y_test, y_test_prob_et)\n",
    "print(\"Extra Trees AUC Score:\", auc(fpr_et, tpr_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histgradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "# HistGradientBoosting\n",
    "\n",
    "# Define hyperparameters to tune for HistGradientBoosting\n",
    "hgb_params = {\n",
    "    'max_iter': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'min_samples_leaf': [5, 10, 20]\n",
    "}\n",
    "\n",
    "# Initialize HistGradientBoosting classifier\n",
    "clf_hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for HistGradientBoosting\n",
    "grid_search_hgb = GridSearchCV(clf_hgb, hgb_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_hgb.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for HistGradientBoosting\n",
    "print(f\"HistGradientBoosting Best Parameters: {grid_search_hgb.best_params_}\")\n",
    "print(f\"HistGradientBoosting Best Cross-Validation Accuracy: {grid_search_hgb.best_score_}\")\n",
    "\n",
    "# Use the best HistGradientBoosting model to make predictions on the test set\n",
    "best_hgb = grid_search_hgb.best_estimator_\n",
    "y_test_preds_hgb = best_hgb.predict(X_test)\n",
    "\n",
    "# Evaluating the HistGradientBoosting model\n",
    "\n",
    "# Accuracy\n",
    "print(\"HistGradientBoosting Validation Accuracy:\", accuracy_score(y_test, y_test_preds_hgb))\n",
    "\n",
    "# Precision\n",
    "print(\"HistGradientBoosting Validation Precision:\", precision_score(y_test, y_test_preds_hgb))\n",
    "\n",
    "# Recall\n",
    "print(\"HistGradientBoosting Validation Recall:\", recall_score(y_test, y_test_preds_hgb))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_hgb = best_hgb.predict_proba(X_test)[:, 1]\n",
    "print(\"HistGradientBoosting ROC-AUC:\", roc_auc_score(y_test, y_test_prob_hgb))\n",
    "\n",
    "# Log Loss\n",
    "print(\"HistGradientBoosting Log Loss:\", log_loss(y_test, y_test_prob_hgb))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_hgb, tpr_hgb, _ = roc_curve(y_test, y_test_prob_hgb)\n",
    "print(\"HistGradientBoosting AUC Score:\", auc(fpr_hgb, tpr_hgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for Gradient Boosting Classifier\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "clf_gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for Gradient Boosting Classifier\n",
    "grid_search_gb = GridSearchCV(clf_gb, gb_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_gb.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for Gradient Boosting Classifier\n",
    "print(f\"Gradient Boosting Best Parameters: {grid_search_gb.best_params_}\")\n",
    "print(f\"Gradient Boosting Best Cross-Validation Accuracy: {grid_search_gb.best_score_}\")\n",
    "\n",
    "# Use the best Gradient Boosting model to make predictions on the test set\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "y_test_preds_gb = best_gb.predict(X_test)\n",
    "\n",
    "# Evaluating the Gradient Boosting model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Gradient Boosting Validation Accuracy:\", accuracy_score(y_test, y_test_preds_gb))\n",
    "\n",
    "# Precision\n",
    "print(\"Gradient Boosting Validation Precision:\", precision_score(y_test, y_test_preds_gb))\n",
    "\n",
    "# Recall\n",
    "print(\"Gradient Boosting Validation Recall:\", recall_score(y_test, y_test_preds_gb))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_gb = best_gb.predict_proba(X_test)[:, 1]\n",
    "print(\"Gradient Boosting ROC-AUC:\", roc_auc_score(y_test, y_test_prob_gb))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Gradient Boosting Log Loss:\", log_loss(y_test, y_test_prob_gb))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_test_prob_gb)\n",
    "print(\"Gradient Boosting AUC Score:\", auc(fpr_gb, tpr_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import necessary library for AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define hyperparameters to tune for AdaBoost\n",
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Initialize AdaBoost classifier\n",
    "clf_ada = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for AdaBoost\n",
    "grid_search_ada = GridSearchCV(clf_ada, ada_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_ada.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for AdaBoost\n",
    "print(f\"AdaBoost Best Parameters: {grid_search_ada.best_params_}\")\n",
    "print(f\"AdaBoost Best Cross-Validation Accuracy: {grid_search_ada.best_score_}\")\n",
    "\n",
    "# Use the best AdaBoost model to make predictions on the test set\n",
    "best_ada = grid_search_ada.best_estimator_\n",
    "y_test_preds_ada = best_ada.predict(X_test)\n",
    "\n",
    "# Evaluating the AdaBoost model\n",
    "\n",
    "# Accuracy\n",
    "print(\"AdaBoost Validation Accuracy:\", accuracy_score(y_test, y_test_preds_ada))\n",
    "\n",
    "# Precision\n",
    "print(\"AdaBoost Validation Precision:\", precision_score(y_test, y_test_preds_ada))\n",
    "\n",
    "# Recall\n",
    "print(\"AdaBoost Validation Recall:\", recall_score(y_test, y_test_preds_ada))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_ada = best_ada.predict_proba(X_test)[:, 1]\n",
    "print(\"AdaBoost ROC-AUC:\", roc_auc_score(y_test, y_test_prob_ada))\n",
    "\n",
    "# Log Loss\n",
    "print(\"AdaBoost Log Loss:\", log_loss(y_test, y_test_prob_ada))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_ada, tpr_ada, _ = roc_curve(y_test, y_test_prob_ada)\n",
    "print(\"AdaBoost AUC Score:\", auc(fpr_ada, tpr_ada))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for Bagging Classifier\n",
    "bagging_params = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'bootstrap_features': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize Bagging classifier\n",
    "clf_bagging = BaggingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for Bagging Classifier\n",
    "grid_search_bagging = GridSearchCV(clf_bagging, bagging_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_bagging.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for Bagging Classifier\n",
    "print(f\"Bagging Classifier Best Parameters: {grid_search_bagging.best_params_}\")\n",
    "print(f\"Bagging Classifier Best Cross-Validation Accuracy: {grid_search_bagging.best_score_}\")\n",
    "\n",
    "# Use the best Bagging model to make predictions on the test set\n",
    "best_bagging = grid_search_bagging.best_estimator_\n",
    "y_test_preds_bagging = best_bagging.predict(X_test)\n",
    "\n",
    "# Evaluating the Bagging model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Bagging Classifier Validation Accuracy:\", accuracy_score(y_test, y_test_preds_bagging))\n",
    "\n",
    "# Precision\n",
    "print(\"Bagging Classifier Validation Precision:\", precision_score(y_test, y_test_preds_bagging))\n",
    "\n",
    "# Recall\n",
    "print(\"Bagging Classifier Validation Recall:\", recall_score(y_test, y_test_preds_bagging))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_bagging = best_bagging.predict_proba(X_test)[:, 1]\n",
    "print(\"Bagging Classifier ROC-AUC:\", roc_auc_score(y_test, y_test_prob_bagging))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Bagging Classifier Log Loss:\", log_loss(y_test, y_test_prob_bagging))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_bagging, tpr_bagging, _ = roc_curve(y_test, y_test_prob_bagging)\n",
    "print(\"Bagging Classifier AUC Score:\", auc(fpr_bagging, tpr_bagging))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define hyperparameters to tune for CatBoost\n",
    "cb_params = {\n",
    "    'iterations': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "clf_cb = CatBoostClassifier(random_seed=42, verbose=0) # verbose=0 will prevent it from printing intermediate steps\n",
    "\n",
    "# Initialize GridSearchCV for CatBoost\n",
    "grid_search_cb = GridSearchCV(clf_cb, cb_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_cb.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for CatBoost\n",
    "print(f\"CatBoost Best Parameters: {grid_search_cb.best_params_}\")\n",
    "print(f\"CatBoost Best Cross-Validation Accuracy: {grid_search_cb.best_score_}\")\n",
    "\n",
    "# Use the best CatBoost model to make predictions on the test set\n",
    "best_cb = grid_search_cb.best_estimator_\n",
    "y_test_preds_cb = best_cb.predict(X_test)\n",
    "\n",
    "# Evaluating the CatBoost model\n",
    "\n",
    "# Accuracy\n",
    "print(\"CatBoost Validation Accuracy:\", accuracy_score(y_test, y_test_preds_cb))\n",
    "\n",
    "# Precision\n",
    "print(\"CatBoost Validation Precision:\", precision_score(y_test, y_test_preds_cb))\n",
    "\n",
    "# Recall\n",
    "print(\"CatBoost Validation Recall:\", recall_score(y_test, y_test_preds_cb))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_cb = best_cb.predict_proba(X_test)[:, 1]\n",
    "print(\"CatBoost ROC-AUC:\", roc_auc_score(y_test, y_test_prob_cb))\n",
    "\n",
    "# Log Loss\n",
    "print(\"CatBoost Log Loss:\", log_loss(y_test, y_test_prob_cb))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_cb, tpr_cb, _ = roc_curve(y_test, y_test_prob_cb)\n",
    "print(\"CatBoost AUC Score:\", auc(fpr_cb, tpr_cb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "# Initialize a k-fold cross-validator (e.g., 5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "#Create an XGBoost classifier\n",
    "clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # Binary classification\n",
    "    use_label_encoder=False  # Suppress warning about label encoding\n",
    ")\n",
    "\n",
    "#Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': np.arange(0.20, 0.30, 0.01),\n",
    "    'n_estimators': np.arange(205, 209, 1),\n",
    "}\n",
    "\n",
    "#Perform hyperparameter tuning with cross-validation\n",
    "grid_search_xgb = GridSearchCV(estimator=clf, param_grid=param_grid,cv=kf, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_val, y_train_val)\n",
    "\n",
    "#Get the best hyperparameters from the grid search\n",
    "best_params = grid_search_xgb.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "#Train the XGBoost model with the best hyperparameters\n",
    "best_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    **best_params\n",
    ")\n",
    "best_clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "#Make predictions\n",
    "y_test_preds = best_clf.predict(X_test)\n",
    "\n",
    "#Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, best_clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"XGBoost Best Cross-Validation Accuracy: {grid_search_xgb.best_score_}\")\n",
    "\n",
    "#Accuracy\n",
    "print(\"XGBoost Validation Accuracy:\",\n",
    "      accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "#Precision\n",
    "print(\"XGBoost Validation Precision:\",\n",
    "      precision_score(y_test, y_test_preds))\n",
    "\n",
    "#Recall\n",
    "print(\"XGBoost Validation Recall:\", recall_score(y_test, y_test_preds))\n",
    "\n",
    "#ROC-AUC for probability scores\n",
    "y_test_prob = best_clf.predict_proba(X_test)[:, 1]\n",
    "print(\"XGBoost ROC-AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "\n",
    "#Log Loss\n",
    "print(\"XGBoost Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost \n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Best parameters for each model\n",
    "catboost_params = {'depth': 8, 'iterations': 1000, 'l2_leaf_reg': 1, 'learning_rate': 0.05}\n",
    "bagging_params = {'bootstrap': False, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
    "gradient_boosting_params = {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "hist_gradient_boosting_params = {'learning_rate': 0.1, 'max_depth': 15, 'max_iter': 200, 'min_samples_leaf': 10}\n",
    "random_forest_params = {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
    "xgboost_params = {'learning_rate': 0.22000000000000003, 'max_depth': 6, 'n_estimators': 207, 'objective': 'binary:logistic', 'use_label_encoder': False}  # Added XGBoost parameters\n",
    "ada_params =  {'learning_rate': 1, 'n_estimators': 150}\n",
    "\n",
    "# Base classifiers\n",
    "catboost = CatBoostClassifier(**catboost_params, random_seed=42, verbose=0)\n",
    "bagging = BaggingClassifier(**bagging_params, random_state=42)\n",
    "gradient_boosting = GradientBoostingClassifier(**gradient_boosting_params, random_state=42)\n",
    "hist_gradient_boosting = HistGradientBoostingClassifier(**hist_gradient_boosting_params, random_state=42)\n",
    "random_forest = RandomForestClassifier(**random_forest_params, random_state=42)\n",
    "xgboost = XGBClassifier(**xgboost_params)  \n",
    "\n",
    "ada = AdaBoostClassifier(**ada_params,random_state=42)\n",
    "\n",
    "# Updated estimators list\n",
    "estimators = [\n",
    "    ('catboost', catboost),\n",
    "    ('bagging', bagging),\n",
    "    ('gradient_boosting', gradient_boosting),\n",
    "    ('hist_gradient_boosting', hist_gradient_boosting),\n",
    "    ('random_forest', random_forest),\n",
    "    ('xgboost', xgboost),\n",
    "    ('ada',ada)\n",
    "]\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=None, cv=kf, n_jobs=-1)\n",
    "\n",
    "# Assuming you've already defined your training and test sets as X_train_val, y_train_val and X_test, y_test respectively\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train_val_clustered, y_train_val)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_preds_stacking = stacking_clf.predict(X_test_clustered)\n",
    "\n",
    "# Evaluating the Stacking model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Stacking Classifier test Accuracy:\", accuracy_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# Precision\n",
    "print(\"Stacking Classifier test Precision:\", precision_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# Recall\n",
    "print(\"Stacking Classifier test Recall:\", recall_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_stacking = stacking_clf.predict_proba(X_test_clustered)[:, 1]\n",
    "print(\"Stacking Classifier ROC-AUC:\", roc_auc_score(y_test, y_test_prob_stacking))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Stacking Classifier Log Loss:\", log_loss(y_test, y_test_prob_stacking))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_stacking, tpr_stacking, _ = roc_curve(y_test, y_test_prob_stacking)\n",
    "print(\"Stacking Classifier AUC Score:\", auc(fpr_stacking, tpr_stacking))\n",
    "print(\"Stacking Classifier report\", classification_report(y_test,y_test_preds_stacking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_val_clustered shape:\", X_train_val_clustered.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
